{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e775f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.39.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from semantic_kernel import __version__\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827e72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "kernel = Kernel()\n",
    "service_id = \"default\"\n",
    "\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(service_id=service_id, \n",
    "                        deployment_name=os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "                        endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "                        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b86de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n",
      "It can be downloaded at https://aka.ms/vs/17/release/vc_redist.x64.exe\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\CheekireddyDhaminiMA\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### Adding Hugging face plugins\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_kernel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Kernel\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhugging_face\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceTextCompletion, HuggingFaceTextEmbedding\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore_plugins\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextMemoryPlugin\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SemanticTextMemory, VolatileMemoryStore\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\semantic_kernel\\connectors\\ai\\hugging_face\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright (c) Microsoft. All rights reserved.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhugging_face\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhf_prompt_execution_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     HuggingFacePromptExecutionSettings,\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhugging_face\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhf_text_completion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     HuggingFaceTextCompletion,\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhugging_face\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mservices\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhf_text_embedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     HuggingFaceTextEmbedding,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m __all__ = [\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHuggingFacePromptExecutionSettings\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHuggingFaceTextCompletion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHuggingFaceTextEmbedding\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\semantic_kernel\\connectors\\ai\\hugging_face\\services\\hf_text_completion.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m override  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, TextIteratorStreamer, pipeline\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_kernel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnectors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhugging_face\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhf_prompt_execution_settings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePromptExecutionSettings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py:277\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    273\u001b[39m             err = ctypes.WinError(ctypes.get_last_error())\n\u001b[32m    274\u001b[39m             err.strerror += (\n\u001b[32m    275\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    276\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m kernel32.SetErrorMode(prev_error_mode)\n",
      "\u001b[31mOSError\u001b[39m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\CheekireddyDhaminiMA\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "### Adding Hugging face plugins\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextCompletion, HuggingFaceTextEmbedding\n",
    "from semantic_kernel.core_plugins import TextMemoryPlugin\n",
    "from semantic_kernel.memory import SemanticTextMemory, VolatileMemoryStore\n",
    "\n",
    "kernel = Kernel()\n",
    "text_service_id = \"HuggingFaceM4/tiny-random-LlamaForCausalLM\"\n",
    "kernel.add_service(\n",
    "    service=HuggingFaceTextCompletion(\n",
    "        service_id=text_service_id, ai_model_id=text_service_id, task=\"text-generation\"\n",
    "    ),\n",
    ")\n",
    "embed_service_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_svc = HuggingFaceTextEmbedding(service_id=embed_service_id, ai_model_id=embed_service_id)\n",
    "kernel.add_service(\n",
    "    service=embedding_svc,\n",
    ")\n",
    "memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=embedding_svc)\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7173509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CheekireddyDhaminiMA\\AppData\\Local\\Temp\\ipykernel_19176\\1906385738.py:37: DeprecationWarning: This class will be removed in a future version. Please use the InMemoryStore and Collection instead.\n",
      "  memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=embedding_svc)\n",
      "C:\\Users\\CheekireddyDhaminiMA\\AppData\\Local\\Temp\\ipykernel_19176\\1906385738.py:37: DeprecationWarning: This class will be removed in a future version.\n",
      "  memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=embedding_svc)\n",
      "C:\\Users\\CheekireddyDhaminiMA\\AppData\\Local\\Temp\\ipykernel_19176\\1906385738.py:38: DeprecationWarning: This class is deprecated and will be removed in a future version. Use the new `collection.as_text_search` instead.\n",
      "  kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memories saved to Volatile Store.\n",
      "\n",
      "User: Compare sharks and whales.\n",
      "ChatBot:> Certainly! Here’s a comparison between sharks and whales:\n",
      "\n",
      "- **Classification**: Sharks are fish, specifically cartilaginous fish, meaning their skeletons are made of cartilage rather than bone. Whales are mammals, which means they are warm-blooded, breathe air through lungs, and nurse their young with milk.\n",
      "\n",
      "- **Respiration**: Sharks breathe through gills, extracting oxygen from water. Whales breathe air using lungs and must come to the surface regularly to breathe through their blow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.core_plugins import TextMemoryPlugin\n",
    "from semantic_kernel.memory import SemanticTextMemory, VolatileMemoryStore\n",
    "from semantic_kernel.prompt_template import PromptTemplateConfig\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "# 1️⃣ Configure Azure OpenAI Services\n",
    "chat_service_id = \"azure_chat\"\n",
    "embed_service_id = \"azure_embed\"\n",
    "\n",
    "# Add Chat Service (GPT)\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        service_id=chat_service_id,\n",
    "        deployment_name=\"gpt-35-turbo\", # Replace with your deployment name\n",
    "        endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add Embedding Service (Ada)\n",
    "embedding_svc = AzureTextEmbedding(\n",
    "    service_id=embed_service_id,\n",
    "    deployment_name=\"text-embedding-ada-002\", # Replace with your deployment name\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "kernel.add_service(embedding_svc)\n",
    "\n",
    "# 2️⃣ Configure Memory and the TextMemoryPlugin\n",
    "# This connects the VolatileStore + Azure Embeddings to the Kernel\n",
    "memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=embedding_svc)\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n",
    "\n",
    "async def main():\n",
    "    # 3️⃣ Add Memories\n",
    "    collection_id = \"generic\"\n",
    "    await memory.save_information(collection=collection_id, id=\"info1\", text=\"Sharks are fish.\")\n",
    "    await memory.save_information(collection=collection_id, id=\"info2\", text=\"Whales are mammals.\")\n",
    "    await memory.save_information(collection=collection_id, id=\"info3\", text=\"Penguins are birds.\")\n",
    "    \n",
    "    print(\"✅ Memories saved to Volatile Store.\")\n",
    "\n",
    "    # 4️⃣ Define Prompt using {{recall}}\n",
    "    # The 'recall' function looks into the memory automatically\n",
    "    my_prompt = \"\"\"\n",
    "    I know these animal facts: \n",
    "    - {{recall 'sharks'}}\n",
    "    - {{recall 'whales'}} \n",
    "    - {{recall 'penguins'}}\n",
    "\n",
    "    Now, tell me something about: {{$request}}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    execution_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=chat_service_id,\n",
    "        max_tokens=100,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    prompt_template_config = PromptTemplateConfig(\n",
    "        template=my_prompt,\n",
    "        template_format=\"semantic-kernel\",\n",
    "        execution_settings=execution_settings,\n",
    "    )\n",
    "\n",
    "    my_function = kernel.add_function(\n",
    "        function_name=\"animal_chat\",\n",
    "        plugin_name=\"AnimalPlugin\",\n",
    "        prompt_template_config=prompt_template_config,\n",
    "    )\n",
    "\n",
    "    # 5️⃣ Run the Chat\n",
    "    user_input = \"Compare sharks and whales.\"\n",
    "    result = await kernel.invoke(my_function, request=user_input)\n",
    "    \n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    print(f\"ChatBot:> {result}\")\n",
    "\n",
    "# Run\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01fec07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Native funtions - a regular function, which is indicated using @kernel_function\n",
    "## A Native function with multiple functions\n",
    "\n",
    "from semantic_kernel.functions import kernel_function, KernelArguments\n",
    "\n",
    "class WeatherPlugin:\n",
    "    @kernel_function(\n",
    "        name=\"get_weather\",\n",
    "        description=\"Gets the current weather report for a specific country\"\n",
    "    )\n",
    "    def get_weather(self, city: str) -> str:\n",
    "        if \"London\" in city:\n",
    "            return \"15 degrees and rainy.\"\n",
    "        return \"22 degrees and sunny\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771edb19",
   "metadata": {},
   "source": [
    "##### Manul Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def manual_invoke_function():\n",
    "    city_input=\"London\"\n",
    "    kernel.add_plugin(WeatherPlugin(), plugin_name=\"Weather\")\n",
    "    result = await kernel.invoke(\n",
    "        plugin_name=\"Weather\", \n",
    "        function_name=\"get_weather\",\n",
    "        arguments=KernelArguments(city=city_input),\n",
    "    )\n",
    "    print(\"Result:  \",result)\n",
    "\n",
    "await manual_invoke_function()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54239c4",
   "metadata": {},
   "source": [
    "#### Auto Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d80204f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "\n",
    "async def auto_invoke_example():\n",
    "    \n",
    "    kernel.add_service(AzureChatCompletion(\n",
    "        service_id=\"test_now\",\n",
    "        deployment_name=\"gpt-4.1\",\n",
    "        endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    ))\n",
    "    settings = kernel.get_prompt_execution_settings_from_service_id(\"test_now\")\n",
    "    settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "    user_query = \"I'm going to London tomorrow. What's the weather like?\"\n",
    "\n",
    "    response = await kernel.invoke_prompt(\n",
    "        prompt=user_query,\n",
    "        settings=settings,\n",
    "    )\n",
    "\n",
    "    print(\"AI Response: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:  The weather in London tomorrow will be 15°C with rain. You may want to bring an umbrella or raincoat!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "await auto_invoke_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff1f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
